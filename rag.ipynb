{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"genai-principles.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Karan Singh, Assistant Professor of Operations Research \n",
      "Principles of Generative AI \n",
      "A Technical Introduction \n",
      "Generative artificial intelligence (GenAI) tools are an emerging class of new-age arti\n",
      "{'source': 'genai-principles.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[0:200])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Memory vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vector_store = InMemoryVectorStore.from_documents(\n",
    "    documents=splits, embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What images are present in this document?',\n",
       " 'context': [Document(id='98132e5e-0879-4ac7-b889-087a5beed791', metadata={'source': 'genai-principles.pdf', 'page': 1}, page_content='product deployment, for example, by Adobe for creating visual content and by Github as a \\nprogramming assistance tool.   \\n 2\\nFigure 2: An image-\\nbased GenAI model, \\nMidjourney’s response to \\nthe prompt — \\n“Businessman in Tokyo \\namidst rush hour, his \\ngaze fixed ahead, \\nsurrounded by a sea of \\nblack umbrellas.”\\nFigure 3: Based on a code-based GenAI model, OpenAI Codex, \\nGithub Copilot is a commercial tool that can generate functional \\ncode from specifications given as natural language. Reportedly, as \\nof June 2023, it served over a million users.'),\n",
       "  Document(id='1b8c1368-0d41-457d-b66d-63fff9fe0709', metadata={'source': 'genai-principles.pdf', 'page': 6}, page_content='representations. This permits us to (a) train a deep net to separate images of cats and dogs on \\na large dataset and (b) subsequently build a shallow (even linear) performant neural net that \\nuses the lower layers of the former to craft useful representations to classify images of zebra \\nand giraffes. Step A is often called pre-training, and step B is referred to as supervised fine-\\ntuning. This manner of amortizing the learning across tasks that are not individually data-rich is \\ncentral to language models. \\nWord Embeddings and Contrastive Learning \\nWhile the progress of deep learning in speech and audio was made possible by the availability \\nof large crowd-labeled datasets (with 10s of millions of annotated images), such large high-\\nquality datasets were absent in the textual domain, despite a plethora of unlabelled data in the \\nform of books, Wikipedia articles, and articles on the internet. Could a machine learning \\nalgorithm make use of the cheap, unlabelled data instead?'),\n",
       "  Document(id='04a5b834-b87a-4dbc-8d63-f18403d30a58', metadata={'source': 'genai-principles.pdf', 'page': 6}, page_content='vector closest to King - Man + Woman. Thus, each vector dimension captured some abstract \\nsemantic degree of freedom. These representations were also valuable for natural classification \\ntasks with limited data, such as sentiment classification, given a small number of examples. \\n 7'),\n",
       "  Document(id='317720c4-8e28-4fbb-a13f-586beda920fb', metadata={'source': 'genai-principles.pdf', 'page': 5}, page_content='6\\nFigure 6: A typical deep neural network for recognizing faces. Each \\nsuccessive layer progressively learns higher-level representations (from \\nedges to contours to faces).')],\n",
       " 'answer': 'The document contains an image generated by Midjourney in response to the prompt “Businessman in Tokyo amidst rush hour, his gaze fixed ahead, surrounded by a sea of black umbrellas.”'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, qa_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What images are present in this document?\"})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'context', 'answer'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document contains an image generated by Midjourney in response to the prompt “Businessman in Tokyo amidst rush hour, his gaze fixed ahead, surrounded by a sea of black umbrellas.”\n"
     ]
    }
   ],
   "source": [
    "print(results[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
